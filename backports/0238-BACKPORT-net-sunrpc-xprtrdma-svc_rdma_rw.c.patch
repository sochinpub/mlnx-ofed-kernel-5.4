From: Tom Wu <tomwu@nvidia.com>
Subject: [PATCH] BACKPORT: net/sunrpc/xprtrdma/svc_rdma_rw.c

Change-Id: I9288219ffc2331e8389be20dd8986156ca0d3b23
---
 net/sunrpc/xprtrdma/svc_rdma_rw.c | 80 +++++++++++++++++++++++++++++++++++++++
 1 file changed, 80 insertions(+)

--- a/net/sunrpc/xprtrdma/svc_rdma_rw.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_rw.c
@@ -12,7 +12,9 @@
 #include <linux/sunrpc/svc_rdma.h>
 
 #include "xprt_rdma.h"
+#ifdef HAVE_TRACE_RPCRDMA_H
 #include <trace/events/rpcrdma.h>
+#endif
 
 static void svc_rdma_write_done(struct ib_cq *cq, struct ib_wc *wc);
 static void svc_rdma_wc_read_done(struct ib_cq *cq, struct ib_wc *wc);
@@ -70,23 +72,37 @@ svc_rdma_get_rw_ctxt(struct svcxprt_rdma
 	}
 
 	ctxt->rw_sg_table.sgl = ctxt->rw_first_sgl;
+#ifdef HAVE_SG_ALLOC_TABLE_CHAINED_NENTS_FIRST_CHUNK_PARAM
 	if (sg_alloc_table_chained(&ctxt->rw_sg_table, sges,
 				   ctxt->rw_sg_table.sgl,
 				   SG_CHUNK_SIZE))
+#else
+	if (sg_alloc_table_chained(&ctxt->rw_sg_table, sges,
+#ifdef HAVE_SG_ALLOC_TABLE_CHAINED_GFP_MASK
+				   GFP_ATOMIC,
+#endif
+				   ctxt->rw_sg_table.sgl))
+#endif
 		goto out_free;
 	return ctxt;
 
 out_free:
 	kfree(ctxt);
 out_noctx:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_no_rwctx_err(rdma, sges);
+#endif
 	return NULL;
 }
 
 static void svc_rdma_put_rw_ctxt(struct svcxprt_rdma *rdma,
 				 struct svc_rdma_rw_ctxt *ctxt)
 {
+#ifdef HAVE_SG_ALLOC_TABLE_CHAINED_NENTS_FIRST_CHUNK_PARAM
 	sg_free_table_chained(&ctxt->rw_sg_table, SG_CHUNK_SIZE);
+#else
+	sg_free_table_chained(&ctxt->rw_sg_table, true);
+#endif
 
 	spin_lock(&rdma->sc_rw_ctxt_lock);
 	list_add(&ctxt->rw_list, &rdma->sc_rw_ctxts);
@@ -131,7 +147,9 @@ static int svc_rdma_rw_ctx_init(struct s
 			       0, offset, handle, direction);
 	if (unlikely(ret < 0)) {
 		svc_rdma_put_rw_ctxt(rdma, ctxt);
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_svcrdma_dma_map_rw_err(rdma, ctxt->rw_nents, ret);
+#endif
 	}
 	return ret;
 }
@@ -244,7 +262,9 @@ static void svc_rdma_write_done(struct i
 	struct svc_rdma_write_info *info =
 			container_of(cc, struct svc_rdma_write_info, wi_cc);
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_wc_write(wc, &cc->cc_cid);
+#endif
 
 	atomic_add(cc->cc_sqecount, &rdma->sc_sq_avail);
 	wake_up(&rdma->sc_send_wait);
@@ -302,7 +322,9 @@ static void svc_rdma_wc_read_done(struct
 	struct svc_rdma_read_info *info =
 			container_of(cc, struct svc_rdma_read_info, ri_cc);
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_wc_read(wc, &cc->cc_cid);
+#endif
 
 	atomic_add(cc->cc_sqecount, &rdma->sc_sq_avail);
 	wake_up(&rdma->sc_send_wait);
@@ -358,21 +380,29 @@ static int svc_rdma_post_chunk_ctxt(stru
 	do {
 		if (atomic_sub_return(cc->cc_sqecount,
 				      &rdma->sc_sq_avail) > 0) {
+#ifdef HAVE_TRACE_RPCRDMA_H
 			trace_svcrdma_post_chunk(&cc->cc_cid, cc->cc_sqecount);
+#endif
 			ret = ib_post_send(rdma->sc_qp, first_wr, &bad_wr);
 			if (ret)
 				break;
 			return 0;
 		}
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_svcrdma_sq_full(rdma);
+#endif
 		atomic_add(cc->cc_sqecount, &rdma->sc_sq_avail);
 		wait_event(rdma->sc_send_wait,
 			   atomic_read(&rdma->sc_sq_avail) > cc->cc_sqecount);
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_svcrdma_sq_retry(rdma);
+#endif
 	} while (1);
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_sq_post_err(rdma, ret);
+#endif
 	set_bit(XPT_CLOSE, &xprt->xpt_flags);
 
 	/* If even one was posted, there will be a completion. */
@@ -471,7 +501,9 @@ svc_rdma_build_writes(struct svc_rdma_wr
 		if (ret < 0)
 			return -EIO;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_svcrdma_send_wseg(handle, write_len, offset);
+#endif
 
 		list_add(&ctxt->rw_list, &cc->cc_rwctxts);
 		cc->cc_sqecount += ret;
@@ -488,8 +520,10 @@ svc_rdma_build_writes(struct svc_rdma_wr
 	return 0;
 
 out_overflow:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_small_wrch_err(rdma, remaining, info->wi_seg_no,
 				     info->wi_nsegs);
+#endif
 	return -E2BIG;
 }
 
@@ -558,7 +592,9 @@ int svc_rdma_send_write_chunk(struct svc
 	if (ret < 0)
 		goto out_err;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_send_write_chunk(xdr->page_len);
+#endif
 	return length;
 
 out_err:
@@ -618,7 +654,9 @@ int svc_rdma_send_reply_chunk(struct svc
 	if (ret < 0)
 		goto out_err;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_send_reply_chunk(consumed);
+#endif
 	return consumed;
 
 out_err:
@@ -680,7 +718,9 @@ static int svc_rdma_build_read_segment(s
 	return 0;
 
 out_overrun:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_page_overrun_err(cc->cc_rdma, rqstp, info->ri_pageno);
+#endif
 	return -EINVAL;
 }
 
@@ -705,7 +745,9 @@ static int svc_rdma_build_read_chunk(str
 		if (ret < 0)
 			break;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_svcrdma_send_rseg(handle, length, offset);
+#endif
 		info->ri_chunklen += length;
 	}
 
@@ -726,13 +768,22 @@ static int svc_rdma_build_normal_read_ch
 	struct svc_rdma_recv_ctxt *head = info->ri_readctxt;
 	int ret;
 
+#ifndef HAVE_SVC_FILL_WRITE_VECTOR
+	info->ri_pageno = head->rc_hdr_count;
+	info->ri_pageoff = 0;
+#endif
+
 	ret = svc_rdma_build_read_chunk(rqstp, info, p);
 	if (ret < 0)
 		goto out;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_send_read_chunk(info->ri_chunklen, info->ri_position);
+#endif
 
+#ifdef HAVE_SVC_FILL_WRITE_VECTOR
 	head->rc_hdr_count = 0;
+#endif
 
 	/* Split the Receive buffer between the head and tail
 	 * buffers at Read chunk's position. XDR roundup of the
@@ -782,15 +833,23 @@ static int svc_rdma_build_pz_read_chunk(
 	struct svc_rdma_recv_ctxt *head = info->ri_readctxt;
 	int ret;
 
+#ifndef HAVE_SVC_FILL_WRITE_VECTOR
+	info->ri_pageno = head->rc_hdr_count - 1;
+	info->ri_pageoff = offset_in_page(head->rc_byte_len);
+#endif
+
 	ret = svc_rdma_build_read_chunk(rqstp, info, p);
 	if (ret < 0)
 		goto out;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_svcrdma_send_pzr(info->ri_chunklen);
+#endif
 
 	head->rc_arg.len += info->ri_chunklen;
 	head->rc_arg.buflen += info->ri_chunklen;
 
+#ifdef HAVE_SVC_FILL_WRITE_VECTOR
 	head->rc_hdr_count = 1;
 	head->rc_arg.head[0].iov_base = page_address(head->rc_pages[0]);
 	head->rc_arg.head[0].iov_len = min_t(size_t, PAGE_SIZE,
@@ -798,6 +857,22 @@ static int svc_rdma_build_pz_read_chunk(
 
 	head->rc_arg.page_len = info->ri_chunklen -
 				head->rc_arg.head[0].iov_len;
+#else
+	if (head->rc_arg.buflen <= head->rc_sges[0].length) {
+		/* Transport header and RPC message fit entirely
+		 * in page where head iovec resides.
+		 */
+		head->rc_arg.head[0].iov_len = info->ri_chunklen;
+	} else {
+		/* Transport header and part of RPC message reside
+		 * in the head iovec's page.
+		 */
+		head->rc_arg.head[0].iov_len =
+			head->rc_sges[0].length - head->rc_byte_len;
+		head->rc_arg.page_len =
+			info->ri_chunklen - head->rc_arg.head[0].iov_len;
+	}
+#endif
 
 out:
 	return ret;
@@ -850,6 +925,9 @@ int svc_rdma_recv_read_chunk(struct svcx
 	 * head->rc_arg. Pages involved with RDMA Read I/O are
 	 * transferred there.
 	 */
+#ifndef HAVE_SVC_FILL_WRITE_VECTOR
+	head->rc_page_count = head->rc_hdr_count;
+#endif
 	head->rc_arg.head[0] = rqstp->rq_arg.head[0];
 	head->rc_arg.tail[0] = rqstp->rq_arg.tail[0];
 	head->rc_arg.pages = head->rc_pages;
@@ -862,8 +940,10 @@ int svc_rdma_recv_read_chunk(struct svcx
 	if (!info)
 		return -ENOMEM;
 	info->ri_readctxt = head;
+#ifdef HAVE_SVC_FILL_WRITE_VECTOR
 	info->ri_pageno = 0;
 	info->ri_pageoff = 0;
+#endif
 
 	info->ri_position = be32_to_cpup(p + 1);
 	if (info->ri_position)
