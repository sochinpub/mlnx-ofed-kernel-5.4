From: Sergey Gorenko <sergeygo@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/infiniband/ulp/iser/iser_initiator.c

Change-Id: I433e1c81ffa2672be7ae32960ffd6a5cf48c1454
---
 drivers/infiniband/ulp/iser/iser_initiator.c | 88 ++++++++++++++++++--
 1 file changed, 81 insertions(+), 7 deletions(-)

--- a/drivers/infiniband/ulp/iser/iser_initiator.c
+++ b/drivers/infiniband/ulp/iser/iser_initiator.c
@@ -37,7 +37,9 @@
 #include <linux/kfifo.h>
 #include <scsi/scsi_cmnd.h>
 #include <scsi/scsi_host.h>
-
+#ifndef HAVE_ISCSI_TRANSPORT_CHECK_PROTECTION
+#include <scsi/scsi_eh.h>
+#endif
 #include "iscsi_iser.h"
 
 /* Register user buffer memory and initialize passive rdma
@@ -650,6 +652,45 @@ iser_check_remote_inv(struct iser_conn *
 	return 0;
 }
 
+#ifndef HAVE_ISCSI_TRANSPORT_CHECK_PROTECTION
+static void iser_check_protection(struct iscsi_conn *conn,
+                                  struct iscsi_hdr *hdr)
+{
+        struct iscsi_task *task;
+        struct iscsi_iser_task *iser_task;
+        struct scsi_cmnd *sc;
+        enum iser_data_dir dir;
+        sector_t sector;
+        u8 ascq;
+
+#ifndef CONFIG_COMPAT_ISCSI_SESSION_FRWD_LOCK
+        spin_lock(&conn->session->lock);
+        task = iscsi_itt_to_ctask(conn, hdr->itt);
+        spin_unlock(&conn->session->lock);
+#else
+        spin_lock(&conn->session->back_lock);
+        task = iscsi_itt_to_ctask(conn, hdr->itt);
+        spin_unlock(&conn->session->back_lock);
+#endif
+        sc = task->sc;
+        iser_task = task->dd_data;
+
+        dir = iser_task->dir[ISER_DIR_IN] ? ISER_DIR_IN : ISER_DIR_OUT;
+        ascq = iser_check_task_pi_status(iser_task, dir, &sector);
+        if (ascq) {
+                sc->result = DRIVER_SENSE << 24 | DID_ABORT << 16 |
+                             SAM_STAT_CHECK_CONDITION;
+                scsi_build_sense_buffer(1, sc->sense_buffer,
+                                        ILLEGAL_REQUEST, 0x10, ascq);
+                sc->sense_buffer[7] = 0xc; /* Additional sense length */
+                sc->sense_buffer[8] = 0;   /* Information desc type */
+                sc->sense_buffer[9] = 0xa; /* Additional desc length */
+                sc->sense_buffer[10] = 0x80; /* Validity bit */
+
+                put_unaligned_be64(sector, &sc->sense_buffer[12]);
+        }
+}
+#endif
 
 void iser_task_rsp(struct ib_cq *cq, struct ib_wc *wc)
 {
@@ -675,6 +716,12 @@ void iser_task_rsp(struct ib_cq *cq, str
 	iser_dbg("op 0x%x itt 0x%x dlen %d\n", hdr->opcode,
 		 hdr->itt, length);
 
+#ifndef HAVE_ISCSI_TRANSPORT_CHECK_PROTECTION
+        if (hdr->opcode == ISCSI_OP_SCSI_CMD_RSP &&
+            ib_conn->pi_support)
+		iser_check_protection(iser_conn->iscsi_conn, hdr);
+#endif
+
 	if (iser_check_remote_inv(iser_conn, wc, hdr)) {
 		iscsi_conn_failure(iser_conn->iscsi_conn,
 				   ISCSI_ERR_CONN_FAILED);
@@ -765,12 +812,27 @@ void iser_task_rdma_init(struct iscsi_is
 void iser_task_rdma_finalize(struct iscsi_iser_task *iser_task)
 {
 	int prot_count = scsi_prot_sg_count(iser_task->sc);
+#ifndef HAVE_VIRT_BOUNDARY
+	bool is_rdma_data_aligned;
+#endif
 
 	if (iser_task->dir[ISER_DIR_IN]) {
+#ifndef HAVE_VIRT_BOUNDARY
+		is_rdma_data_aligned = true;
+		if (iser_task->data[ISER_DIR_IN].orig_sg) {
+			iser_finalize_rdma_unaligned_sg(iser_task,
+							&iser_task->data[ISER_DIR_IN],
+							ISER_DIR_IN);
+			is_rdma_data_aligned = false;
+		}
+#endif
 		iser_unreg_mem_fastreg(iser_task, ISER_DIR_IN);
-		iser_dma_unmap_task_data(iser_task,
-					 &iser_task->data[ISER_DIR_IN],
-					 DMA_FROM_DEVICE);
+#ifndef HAVE_VIRT_BOUNDARY
+		if (is_rdma_data_aligned)
+#endif
+			iser_dma_unmap_task_data(iser_task,
+						 &iser_task->data[ISER_DIR_IN],
+						 DMA_FROM_DEVICE);
 		if (prot_count)
 			iser_dma_unmap_task_data(iser_task,
 						 &iser_task->prot[ISER_DIR_IN],
@@ -778,10 +840,22 @@ void iser_task_rdma_finalize(struct iscs
 	}
 
 	if (iser_task->dir[ISER_DIR_OUT]) {
+#ifndef HAVE_VIRT_BOUNDARY
+		is_rdma_data_aligned = true;
+		if (iser_task->data[ISER_DIR_OUT].orig_sg) {
+			iser_finalize_rdma_unaligned_sg(iser_task,
+							&iser_task->data[ISER_DIR_OUT],
+							ISER_DIR_OUT);
+			is_rdma_data_aligned = false;
+		}
+#endif
 		iser_unreg_mem_fastreg(iser_task, ISER_DIR_OUT);
-		iser_dma_unmap_task_data(iser_task,
-					 &iser_task->data[ISER_DIR_OUT],
-					 DMA_TO_DEVICE);
+#ifndef HAVE_VIRT_BOUNDARY
+		if (is_rdma_data_aligned)
+#endif
+			iser_dma_unmap_task_data(iser_task,
+						 &iser_task->data[ISER_DIR_OUT],
+						 DMA_TO_DEVICE);
 		if (prot_count)
 			iser_dma_unmap_task_data(iser_task,
 						 &iser_task->prot[ISER_DIR_OUT],
