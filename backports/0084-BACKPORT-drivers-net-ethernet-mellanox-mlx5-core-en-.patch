From: Valentine Fatiev <valentinef@mellanox.com>
Subject: [PATCH] BACKPORT:
 drivers/net/ethernet/mellanox/mlx5/core/en/xsk/umem.c

Change-Id: I33d2311d82b3fa9d7c54f222a144e3aeeaddae4a
---
 .../ethernet/mellanox/mlx5/core/en/xsk/umem.c | 65 +++++++++++++++++++
 1 file changed, 65 insertions(+)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xsk/umem.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xsk/umem.c
@@ -1,7 +1,12 @@
 // SPDX-License-Identifier: GPL-2.0 OR Linux-OpenIB
 /* Copyright (c) 2019 Mellanox Technologies. */
 
+#ifdef HAVE_XSK_SUPPORT
+#ifdef HAVE_XDP_SOCK_DRV_H
 #include <net/xdp_sock_drv.h>
+#else
+#include <net/xdp_sock.h>
+#endif
 #include "umem.h"
 #include "setup.h"
 #include "en/params.h"
@@ -10,14 +15,49 @@ static int mlx5e_xsk_map_umem(struct mlx
 			      struct xdp_umem *umem)
 {
 	struct device *dev = priv->mdev->device;
+#ifdef HAVE_XSK_BUFF_ALLOC
 
 	return xsk_buff_dma_map(umem, dev, 0);
+#else
+	u32 i;
+
+	for (i = 0; i < umem->npgs; i++) {
+		dma_addr_t dma = dma_map_page(dev, umem->pgs[i], 0, PAGE_SIZE,
+					      DMA_BIDIRECTIONAL);
+
+		if (unlikely(dma_mapping_error(dev, dma)))
+			goto err_unmap;
+		umem->pages[i].dma = dma;
+	}
+
+	return 0;
+
+err_unmap:
+	while (i--) {
+		dma_unmap_page(dev, umem->pages[i].dma, PAGE_SIZE,
+			       DMA_BIDIRECTIONAL);
+		umem->pages[i].dma = 0;
+	}
+
+	return -ENOMEM;
+#endif
 }
 
 static void mlx5e_xsk_unmap_umem(struct mlx5e_priv *priv,
 				 struct xdp_umem *umem)
 {
+#ifdef HAVE_XSK_BUFF_ALLOC
 	return xsk_buff_dma_unmap(umem, 0);
+#else
+	struct device *dev = priv->mdev->device;
+	u32 i;
+
+	for (i = 0; i < umem->npgs; i++) {
+		dma_unmap_page(dev, umem->pages[i].dma, PAGE_SIZE,
+			       DMA_BIDIRECTIONAL);
+		umem->pages[i].dma = 0;
+	}
+#endif
 }
 
 static int mlx5e_xsk_get_umems(struct mlx5e_xsk *xsk)
@@ -64,14 +104,23 @@ static void mlx5e_xsk_remove_umem(struct
 
 static bool mlx5e_xsk_is_umem_sane(struct xdp_umem *umem)
 {
+#ifdef HAVE_XSK_BUFF_ALLOC
 	return xsk_umem_get_headroom(umem) <= 0xffff &&
 		xsk_umem_get_chunk_size(umem) <= 0xffff;
+#else
+	return umem->headroom <= 0xffff && umem->chunk_size_nohr <= 0xffff;
+#endif
 }
 
 void mlx5e_build_xsk_param(struct xdp_umem *umem, struct mlx5e_xsk_param *xsk)
 {
+#ifdef HAVE_XSK_BUFF_ALLOC
 	xsk->headroom = xsk_umem_get_headroom(umem);
 	xsk->chunk_size = xsk_umem_get_chunk_size(umem);
+#else
+	xsk->headroom = umem->headroom;
+	xsk->chunk_size = umem->chunk_size_nohr + umem->headroom;
+#endif
 }
 
 static int mlx5e_xsk_enable_locked(struct mlx5e_priv *priv,
@@ -215,3 +264,19 @@ int mlx5e_xsk_setup_umem(struct net_devi
 	return umem ? mlx5e_xsk_enable_umem(priv, umem, ix) :
 		      mlx5e_xsk_disable_umem(priv, ix);
 }
+
+#ifndef HAVE_XSK_BUFF_ALLOC
+int mlx5e_xsk_resize_reuseq(struct xdp_umem *umem, u32 nentries)
+{
+	struct xdp_umem_fq_reuse *reuseq;
+
+	reuseq = xsk_reuseq_prepare(nentries);
+	if (unlikely(!reuseq))
+		return -ENOMEM;
+	xsk_reuseq_free(xsk_reuseq_swap(umem, reuseq));
+
+	return 0;
+}
+#endif
+
+#endif /* HAVE_XSK_SUPPORT */
