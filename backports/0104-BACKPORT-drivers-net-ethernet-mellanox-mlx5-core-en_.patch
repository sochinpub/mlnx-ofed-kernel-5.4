From: Mikhael Goikhman <migo@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en_stats.c

Change-Id: If5c3931b4ed0bfdb732435db8a0b5be2ef38686d
---
 .../ethernet/mellanox/mlx5/core/en_stats.c    | 123 ++++++++++++++++--
 1 file changed, 110 insertions(+), 13 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en_stats.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_stats.c
@@ -113,7 +113,7 @@ static const struct counter_desc sw_stat
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_mpwqe_blks) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_mpwqe_pkts) },
 
-#ifdef CONFIG_MLX5_EN_TLS
+#if defined(CONFIG_MLX5_EN_TLS) && defined(HAVE_UAPI_LINUX_TLS_H)
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_tls_encrypted_packets) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_tls_encrypted_bytes) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_tls_ooo) },
@@ -137,8 +137,11 @@ static const struct counter_desc sw_stat
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_csum_complete_tail) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_csum_complete_tail_slow) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_csum_unnecessary_inner) },
+#ifdef HAVE_XDP_BUFF
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_xdp_drop) },
+#ifdef HAVE_XDP_REDIRECT
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_xdp_redirect) },
+#endif
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_xdp_tx_xmit) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_xdp_tx_mpwqe) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_xdp_tx_inlnw) },
@@ -146,6 +149,7 @@ static const struct counter_desc sw_stat
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_xdp_tx_full) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_xdp_tx_err) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_xdp_tx_cqe) },
+#endif
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_csum_none) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_csum_partial) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_csum_partial_inner) },
@@ -156,6 +160,12 @@ static const struct counter_desc sw_stat
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_cqes) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_queue_wake) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_cqe_err) },
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_sw_lro_aggregated) },
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_sw_lro_flushed) },
+	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_sw_lro_no_desc) },
+#endif
+#ifdef HAVE_XDP_REDIRECT
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_xdp_xmit) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_xdp_mpwqe) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_xdp_inlnw) },
@@ -163,6 +173,7 @@ static const struct counter_desc sw_stat
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_xdp_full) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_xdp_err) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_xdp_cqes) },
+#endif
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_cqe_compress_blks) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, tx_cqe_compress_pkts) },
 	{ MLX5E_DECLARE_STAT(struct mlx5e_sw_stats, rx_wqe_err) },
@@ -245,6 +256,27 @@ static MLX5E_DECLARE_STATS_GRP_OP_FILL_S
 	return idx;
 }
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+static void mlx5e_update_sw_lro_stats(struct mlx5e_priv *priv)
+{
+	int i;
+	struct mlx5e_sw_stats *s = &priv->stats.sw;
+
+	s->rx_sw_lro_aggregated = 0;
+	s->rx_sw_lro_flushed = 0;
+	s->rx_sw_lro_no_desc = 0;
+
+	for (i = 0; i < priv->channels.num; i++) {
+		struct mlx5e_sw_lro *sw_lro = &priv->sw_lro[i];
+
+		s->rx_sw_lro_aggregated += sw_lro->lro_mgr.stats.aggregated;
+		s->rx_sw_lro_flushed += sw_lro->lro_mgr.stats.flushed;
+		s->rx_sw_lro_no_desc += sw_lro->lro_mgr.stats.no_desc;
+	}
+}
+#endif
+ 
+
 static MLX5E_DECLARE_STATS_GRP_OP_FILL_STATS(sw)
 {
 	int i;
@@ -254,6 +286,7 @@ static MLX5E_DECLARE_STATS_GRP_OP_FILL_S
 	return idx;
 }
 
+#ifdef HAVE_XDP_REDIRECT
 static void mlx5e_stats_grp_sw_update_stats_xdp_red(struct mlx5e_sw_stats *s,
 						    struct mlx5e_xdpsq_stats *xdpsq_red_stats)
 {
@@ -265,7 +298,9 @@ static void mlx5e_stats_grp_sw_update_st
 	s->tx_xdp_err   += xdpsq_red_stats->err;
 	s->tx_xdp_cqes  += xdpsq_red_stats->cqes;
 }
+#endif
 
+#ifdef HAVE_XDP_BUFF
 static void mlx5e_stats_grp_sw_update_stats_xdpsq(struct mlx5e_sw_stats *s,
 						  struct mlx5e_xdpsq_stats *xdpsq_stats)
 {
@@ -277,7 +312,8 @@ static void mlx5e_stats_grp_sw_update_st
 	s->rx_xdp_tx_err   += xdpsq_stats->err;
 	s->rx_xdp_tx_cqe   += xdpsq_stats->cqes;
 }
-
+#endif
+#ifdef HAVE_XSK_SUPPORT
 static void mlx5e_stats_grp_sw_update_stats_xsksq(struct mlx5e_sw_stats *s,
 						  struct mlx5e_xdpsq_stats *xsksq_stats)
 {
@@ -312,6 +348,7 @@ static void mlx5e_stats_grp_sw_update_st
 	s->rx_xsk_congst_umr             += xskrq_stats->congst_umr;
 	s->rx_xsk_arfs_err               += xskrq_stats->arfs_err;
 }
+#endif
 
 static void mlx5e_stats_grp_sw_update_stats_rq_stats(struct mlx5e_sw_stats *s,
 						     struct mlx5e_rq_stats *rq_stats)
@@ -330,8 +367,12 @@ static void mlx5e_stats_grp_sw_update_st
 	s->rx_csum_complete_tail_slow += rq_stats->csum_complete_tail_slow;
 	s->rx_csum_unnecessary        += rq_stats->csum_unnecessary;
 	s->rx_csum_unnecessary_inner  += rq_stats->csum_unnecessary_inner;
+#ifdef HAVE_XDP_BUFF
 	s->rx_xdp_drop                += rq_stats->xdp_drop;
+#ifdef HAVE_XDP_REDIRECT
 	s->rx_xdp_redirect            += rq_stats->xdp_redirect;
+#endif
+#endif
 	s->rx_wqe_err                 += rq_stats->wqe_err;
 	s->rx_mpwqe_filler_cqes       += rq_stats->mpwqe_filler_cqes;
 	s->rx_mpwqe_filler_strides    += rq_stats->mpwqe_filler_strides;
@@ -398,7 +439,7 @@ static void mlx5e_stats_grp_sw_update_st
 	s->tx_csum_partial_inner    += sq_stats->csum_partial_inner;
 	s->tx_csum_none             += sq_stats->csum_none;
 	s->tx_csum_partial          += sq_stats->csum_partial;
-#ifdef CONFIG_MLX5_EN_TLS
+#if defined(CONFIG_MLX5_EN_TLS) && defined(HAVE_UAPI_LINUX_TLS_H)
 	s->tx_tls_encrypted_packets += sq_stats->tls_encrypted_packets;
 	s->tx_tls_encrypted_bytes   += sq_stats->tls_encrypted_bytes;
 	s->tx_tls_ooo               += sq_stats->tls_ooo;
@@ -445,14 +486,19 @@ static MLX5E_DECLARE_STATS_GRP_OP_UPDATE
 		int j;
 
 		mlx5e_stats_grp_sw_update_stats_rq_stats(s, &channel_stats->rq);
+#ifdef HAVE_XDP_BUFF
 		mlx5e_stats_grp_sw_update_stats_xdpsq(s, &channel_stats->rq_xdpsq);
+#endif
 		mlx5e_stats_grp_sw_update_stats_ch_stats(s, &channel_stats->ch);
+#ifdef HAVE_XDP_REDIRECT
 		/* xdp redirect */
 		mlx5e_stats_grp_sw_update_stats_xdp_red(s, &channel_stats->xdpsq);
+#endif
+#ifdef HAVE_XSK_SUPPORT
 		/* AF_XDP zero-copy */
 		mlx5e_stats_grp_sw_update_stats_xskrq(s, &channel_stats->xskrq);
 		mlx5e_stats_grp_sw_update_stats_xsksq(s, &channel_stats->xsksq);
-
+#endif
 		for (j = 0; j < priv->max_opened_tc; j++) {
 
 			mlx5e_stats_grp_sw_update_stats_sq(s, &channel_stats->sq[j]);
@@ -461,6 +507,9 @@ static MLX5E_DECLARE_STATS_GRP_OP_UPDATE
 		}
 	}
 	mlx5e_stats_grp_sw_update_stats_ptp(priv, s);
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	mlx5e_update_sw_lro_stats(priv);
+#endif
 }
 
 static const struct counter_desc q_stats_desc[] = {
@@ -1561,8 +1610,12 @@ static const struct counter_desc rq_stat
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, csum_unnecessary) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, csum_unnecessary_inner) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, csum_none) },
+#ifdef HAVE_XDP_BUFF
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, xdp_drop) },
+#ifdef HAVE_XDP_REDIRECT
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, xdp_redirect) },
+#endif
+#endif
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, lro_packets) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, lro_bytes) },
 	{ MLX5E_DECLARE_RX_STAT(struct mlx5e_rq_stats, mcast_packets) },
@@ -1641,6 +1694,7 @@ static const struct counter_desc sq_stat
 	{ MLX5E_DECLARE_TX_STAT(struct mlx5e_sq_stats, cqe_err) },
 };
 
+#ifdef HAVE_XDP_BUFF
 static const struct counter_desc rq_xdpsq_stats_desc[] = {
 	{ MLX5E_DECLARE_RQ_XDPSQ_STAT(struct mlx5e_xdpsq_stats, xmit) },
 	{ MLX5E_DECLARE_RQ_XDPSQ_STAT(struct mlx5e_xdpsq_stats, mpwqe) },
@@ -1650,7 +1704,9 @@ static const struct counter_desc rq_xdps
 	{ MLX5E_DECLARE_RQ_XDPSQ_STAT(struct mlx5e_xdpsq_stats, err) },
 	{ MLX5E_DECLARE_RQ_XDPSQ_STAT(struct mlx5e_xdpsq_stats, cqes) },
 };
-
+#endif
+ 
+#ifdef HAVE_XDP_REDIRECT
 static const struct counter_desc xdpsq_stats_desc[] = {
 	{ MLX5E_DECLARE_XDPSQ_STAT(struct mlx5e_xdpsq_stats, xmit) },
 	{ MLX5E_DECLARE_XDPSQ_STAT(struct mlx5e_xdpsq_stats, mpwqe) },
@@ -1660,7 +1716,9 @@ static const struct counter_desc xdpsq_s
 	{ MLX5E_DECLARE_XDPSQ_STAT(struct mlx5e_xdpsq_stats, err) },
 	{ MLX5E_DECLARE_XDPSQ_STAT(struct mlx5e_xdpsq_stats, cqes) },
 };
+#endif
 
+#ifdef HAVE_XSK_SUPPORT
 static const struct counter_desc xskrq_stats_desc[] = {
 	{ MLX5E_DECLARE_XSKRQ_STAT(struct mlx5e_rq_stats, packets) },
 	{ MLX5E_DECLARE_XSKRQ_STAT(struct mlx5e_rq_stats, bytes) },
@@ -1691,6 +1749,7 @@ static const struct counter_desc xsksq_s
 	{ MLX5E_DECLARE_XSKSQ_STAT(struct mlx5e_xdpsq_stats, err) },
 	{ MLX5E_DECLARE_XSKSQ_STAT(struct mlx5e_xdpsq_stats, cqes) },
 };
+#endif
 
 static const struct counter_desc ch_stats_desc[] = {
 	{ MLX5E_DECLARE_CH_STAT(struct mlx5e_ch_stats, events) },
@@ -1734,10 +1793,16 @@ static const struct counter_desc ptp_cq_
 
 #define NUM_RQ_STATS			ARRAY_SIZE(rq_stats_desc)
 #define NUM_SQ_STATS			ARRAY_SIZE(sq_stats_desc)
-#define NUM_XDPSQ_STATS			ARRAY_SIZE(xdpsq_stats_desc)
-#define NUM_RQ_XDPSQ_STATS		ARRAY_SIZE(rq_xdpsq_stats_desc)
+#ifdef HAVE_XSK_SUPPORT
 #define NUM_XSKRQ_STATS			ARRAY_SIZE(xskrq_stats_desc)
 #define NUM_XSKSQ_STATS			ARRAY_SIZE(xsksq_stats_desc)
+#endif
+#ifdef HAVE_XDP_REDIRECT
+#define NUM_XDPSQ_STATS                 ARRAY_SIZE(xdpsq_stats_desc)
+#endif
+#ifdef HAVE_XDP_BUFF
+#define NUM_RQ_XDPSQ_STATS              ARRAY_SIZE(rq_xdpsq_stats_desc)
+#endif
 #define NUM_CH_STATS			ARRAY_SIZE(ch_stats_desc)
 #define NUM_PTP_SQ_STATS		ARRAY_SIZE(ptp_sq_stats_desc)
 #define NUM_PTP_CH_STATS		ARRAY_SIZE(ptp_ch_stats_desc)
@@ -1814,10 +1879,20 @@ static MLX5E_DECLARE_STATS_GRP_OP_NUM_ST
 
 	return (NUM_RQ_STATS * max_nch) +
 	       (NUM_CH_STATS * max_nch) +
-	       (NUM_RQ_XDPSQ_STATS * max_nch) +
+#ifdef HAVE_XDP_BUFF
+              (NUM_RQ_XDPSQ_STATS * max_nch) +
+#ifdef HAVE_XDP_REDIRECT
+              (NUM_XDPSQ_STATS * max_nch) +
+#else
+		0 +
+#endif
+#else
+		0 +
+#endif
+#ifdef HAVE_XSK_SUPPORT
 	       (NUM_XSKRQ_STATS * max_nch * priv->xsk.ever_used) +
 	       (NUM_XSKSQ_STATS * max_nch * priv->xsk.ever_used) +
-	       (NUM_XDPSQ_STATS * max_nch) +
+#endif
 #ifndef CONFIG_MLX5_EN_SPECIAL_SQ
 	       (NUM_SQ_STATS * max_nch * priv->max_opened_tc);
 #else
@@ -1828,7 +1903,9 @@ static MLX5E_DECLARE_STATS_GRP_OP_NUM_ST
 
 static MLX5E_DECLARE_STATS_GRP_OP_FILL_STRS(channels)
 {
+#ifdef HAVE_XSK_SUPPORT
 	bool is_xsk = priv->xsk.ever_used;
+#endif
 	int max_nch = priv->max_nch;
 	int i, j, tc;
 
@@ -1844,12 +1921,17 @@ static MLX5E_DECLARE_STATS_GRP_OP_FILL_S
 		for (j = 0; j < NUM_RQ_STATS; j++)
 			sprintf(data + (idx++) * ETH_GSTRING_LEN,
 				rq_stats_desc[j].format, i);
+#ifdef HAVE_XSK_SUPPORT
 		for (j = 0; j < NUM_XSKRQ_STATS * is_xsk; j++)
 			sprintf(data + (idx++) * ETH_GSTRING_LEN,
 				xskrq_stats_desc[j].format, i);
-		for (j = 0; j < NUM_RQ_XDPSQ_STATS; j++)
-			sprintf(data + (idx++) * ETH_GSTRING_LEN,
-				rq_xdpsq_stats_desc[j].format, i);
+#endif 
+
+#ifdef HAVE_XDP_BUFF
+       	for (j = 0; j < NUM_RQ_XDPSQ_STATS; j++)
+       		sprintf(data + (idx++) * ETH_GSTRING_LEN,
+       			rq_xdpsq_stats_desc[j].format, i);
+#endif
 	}
 
 	for (tc = 0; tc < priv->max_opened_tc; tc++)
@@ -1860,12 +1942,16 @@ static MLX5E_DECLARE_STATS_GRP_OP_FILL_S
 					i + tc * max_nch);
 
 	for (i = 0; i < max_nch; i++) {
+#ifdef HAVE_XSK_SUPPORT
 		for (j = 0; j < NUM_XSKSQ_STATS * is_xsk; j++)
 			sprintf(data + (idx++) * ETH_GSTRING_LEN,
 				xsksq_stats_desc[j].format, i);
+#endif
+#ifdef HAVE_XDP_REDIRECT
 		for (j = 0; j < NUM_XDPSQ_STATS; j++)
 			sprintf(data + (idx++) * ETH_GSTRING_LEN,
 				xdpsq_stats_desc[j].format, i);
+#endif
 	}
 #ifdef CONFIG_MLX5_EN_SPECIAL_SQ
 	/* Special TX queue counters */
@@ -1882,7 +1968,9 @@ static MLX5E_DECLARE_STATS_GRP_OP_FILL_S
 
 static MLX5E_DECLARE_STATS_GRP_OP_FILL_STATS(channels)
 {
+#ifdef HAVE_XSK_SUPPORT
 	bool is_xsk = priv->xsk.ever_used;
+#endif
 	int max_nch = priv->max_nch;
 	int i, j, tc, rlq;
 
@@ -1900,14 +1988,19 @@ static MLX5E_DECLARE_STATS_GRP_OP_FILL_S
 			data[idx++] =
 				MLX5E_READ_CTR64_CPU(&priv->channel_stats[i].rq,
 						     rq_stats_desc, j);
+#ifdef HAVE_XSK_SUPPORT
 		for (j = 0; j < NUM_XSKRQ_STATS * is_xsk; j++)
 			data[idx++] =
 				MLX5E_READ_CTR64_CPU(&priv->channel_stats[i].xskrq,
 						     xskrq_stats_desc, j);
+#endif
+
+#ifdef HAVE_XDP_BUFF
 		for (j = 0; j < NUM_RQ_XDPSQ_STATS; j++)
 			data[idx++] =
 				MLX5E_READ_CTR64_CPU(&priv->channel_stats[i].rq_xdpsq,
 						     rq_xdpsq_stats_desc, j);
+#endif
 	}
 
 	for (tc = 0; tc < priv->max_opened_tc; tc++)
@@ -1916,16 +2009,20 @@ static MLX5E_DECLARE_STATS_GRP_OP_FILL_S
 				data[idx++] =
 					MLX5E_READ_CTR64_CPU(&priv->channel_stats[i].sq[tc],
 							     sq_stats_desc, j);
-
 	for (i = 0; i < max_nch; i++) {
+#ifdef HAVE_XSK_SUPPORT
 		for (j = 0; j < NUM_XSKSQ_STATS * is_xsk; j++)
 			data[idx++] =
 				MLX5E_READ_CTR64_CPU(&priv->channel_stats[i].xsksq,
 						     xsksq_stats_desc, j);
+#endif
+
+#ifdef HAVE_XDP_REDIRECT
 		for (j = 0; j < NUM_XDPSQ_STATS; j++)
 			data[idx++] =
 				MLX5E_READ_CTR64_CPU(&priv->channel_stats[i].xdpsq,
 						     xdpsq_stats_desc, j);
+#endif
 	}
 
 #ifdef CONFIG_MLX5_EN_SPECIAL_SQ
