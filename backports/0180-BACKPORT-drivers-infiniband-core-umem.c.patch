From: Valentine Fatiev <valentinef@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/infiniband/core/umem.c

Change-Id: I4b82b7137e137737f06d1fee07f86e2f483076c9
---
 drivers/infiniband/core/umem.c | 296 +++++++++++++++++++++++++++++++--
 1 file changed, 285 insertions(+), 11 deletions(-)

--- a/drivers/infiniband/core/umem.c
+++ b/drivers/infiniband/core/umem.c
@@ -37,9 +37,15 @@
 #include <linux/sched/signal.h>
 #include <linux/sched/mm.h>
 #include <linux/export.h>
+#include <linux/scatterlist.h>
+#if !defined(HAVE_FOLL_LONGTERM) && !defined(HAVE_GET_USER_PAGES_LONGTERM)
+#include <linux/hugetlb.h>
+#endif
 #include <linux/slab.h>
 #include <linux/pagemap.h>
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 #include <rdma/ib_umem_odp.h>
+#endif
 
 #include "uverbs.h"
 #include "ib_peer_mem.h"
@@ -55,9 +61,21 @@ static void __ib_umem_release(struct ib_
 
 	for_each_sg_page(umem->sg_head.sgl, &sg_iter, umem->sg_nents, 0) {
 		page = sg_page_iter_page(&sg_iter);
+#ifdef HAVE_UNPIN_USER_PAGES_DIRTY_LOCK_EXPORTED
 		unpin_user_pages_dirty_lock(&page, 1, umem->writable && dirty);
+#elif defined(HAVE_PUT_USER_PAGES_DIRTY_LOCK_3_PARAMS)
+		put_user_pages_dirty_lock(&page, 1, umem->writable && dirty);
+#elif defined(HAVE_PUT_USER_PAGES_DIRTY_LOCK_2_PARAMS)
+		if (umem->writable && dirty)
+			put_user_pages_dirty_lock(&page, 1);
+		else
+			put_user_page(page);
+#else
+		if (!PageDirty(page) && umem->writable && dirty)
+                        set_page_dirty_lock(page);
+                put_page(page);
+#endif /*HAVE_UNPIN_USER_PAGES_DIRTY_LOCK_EXPORTED*/
 	}
-
 	sg_free_table(&umem->sg_head);
 }
 
@@ -71,6 +89,7 @@ static void __ib_umem_release(struct ib_
  *
  * Return new end of scatterlist
  */
+#ifndef HAVE_SG_ALLOC_TABLE_FROM_PAGES
 static struct scatterlist *ib_umem_add_sg_table(struct scatterlist *sg,
 						struct page **page_list,
 						unsigned long npages,
@@ -127,6 +146,7 @@ static struct scatterlist *ib_umem_add_s
 
 	return sg;
 }
+#endif
 
 /**
  * ib_umem_find_best_pgsz - Find best HW page size to use for this MR
@@ -191,21 +211,64 @@ EXPORT_SYMBOL(ib_umem_find_best_pgsz);
  * @access: IB_ACCESS_xxx flags for memory being pinned
  * @peer_mem_flags: IB_PEER_MEM_xxx flags for memory being used
  */
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 struct ib_umem *__ib_umem_get(struct ib_device *device,
+#else
+struct ib_umem *__ib_umem_get(struct ib_udata *udata,
+#endif
 			      unsigned long addr, size_t size, int access,
 			      unsigned long peer_mem_flags)
 {
+#ifndef HAVE_MMU_INTERVAL_NOTIFIER
+	struct ib_ucontext *context;
+#endif
 	struct ib_umem *umem;
 	struct page **page_list;
 	unsigned long lock_limit;
+#if defined(HAVE_PINNED_VM) || defined(HAVE_ATOMIC_PINNED_VM)
 	unsigned long new_pinned;
+#endif
 	unsigned long cur_base;
 	unsigned long dma_attr = 0;
 	struct mm_struct *mm;
 	unsigned long npages;
 	int ret;
-	struct scatterlist *sg;
+	struct scatterlist *sg = NULL;
+#ifdef HAVE_GET_USER_PAGES_GUP_FLAGS
 	unsigned int gup_flags = FOLL_WRITE;
+#endif
+#if defined(HAVE_SG_ALLOC_TABLE_FROM_PAGES) && (!defined(HAVE_UNPIN_USER_PAGES_DIRTY_LOCK_EXPORTED) && !defined(HAVE_PUT_USER_PAGES_DIRTY_LOCK_3_PARAMS))
+	unsigned long index;
+#endif
+#if !defined(HAVE_FOLL_LONGTERM) && !defined(HAVE_GET_USER_PAGES_LONGTERM)
+	struct vm_area_struct **vma_list;
+	int i;
+#endif
+#ifdef DMA_ATTR_WRITE_BARRIER
+#ifdef HAVE_STRUCT_DMA_ATTRS
+        DEFINE_DMA_ATTRS(attrs);
+#else
+        unsigned long dma_attrs = 0;
+#endif //HAVE_STRUCT_DMA_ATTRS
+#endif //DMA_ATTR_WRITE_BARRIER
+
+#ifdef DMA_ATTR_WRITE_BARRIER
+#ifdef HAVE_STRUCT_DMA_ATTRS
+	dma_set_attr(DMA_ATTR_WRITE_BARRIER, &attrs);
+#else
+	dma_attrs |= DMA_ATTR_WRITE_BARRIER;
+#endif //HAVE_STRUCT_DMA_ATTRS
+#endif //DMA_ATTR_WRITE_BARRIER
+
+#ifndef HAVE_MMU_INTERVAL_NOTIFIER
+	if (!udata)
+		return ERR_PTR(-EIO);
+
+	context = container_of(udata, struct uverbs_attr_bundle, driver_udata)
+			  ->context;
+	if (!context)
+		return ERR_PTR(-EIO);
+#endif
 
 	/*
 	 * If the combination of the addr and size requested for this memory
@@ -228,19 +291,40 @@ struct ib_umem *__ib_umem_get(struct ib_
 	umem = kzalloc(sizeof(*umem), GFP_KERNEL);
 	if (!umem)
 		return ERR_PTR(-ENOMEM);
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 	umem->ibdev      = device;
+#else
+#ifdef HAVE_MMU_NOTIFIER_OPS_HAS_FREE_NOTIFIER
+	umem->ibdev = context->device;
+#else
+	umem->context = context;
+#endif
+#endif
 	umem->length     = size;
 	umem->address    = addr;
 	umem->writable   = ib_access_writable(access);
 	umem->owning_mm = mm = current->mm;
 	mmgrab(mm);
 
+#if !defined(HAVE_FOLL_LONGTERM) && !defined(HAVE_GET_USER_PAGES_LONGTERM)
+	/* We assume the memory is from hugetlb until proved otherwise */
+	umem->hugetlb   = 1;
+#endif
 	page_list = (struct page **) __get_free_page(GFP_KERNEL);
 	if (!page_list) {
 		ret = -ENOMEM;
 		goto umem_kfree;
 	}
 
+#if !defined(HAVE_FOLL_LONGTERM) && !defined(HAVE_GET_USER_PAGES_LONGTERM)
+	/*
+	 *       * if we can't alloc the vma_list, it's not so bad;
+	 *                 * just assume the memory is not hugetlb memory
+	 *                 */
+	vma_list = (struct vm_area_struct **) __get_free_page(GFP_KERNEL);
+	if (!vma_list)
+		umem->hugetlb = 0;
+#endif
 	npages = ib_umem_num_pages(umem);
 	if (npages == 0 || npages > UINT_MAX) {
 		ret = -EINVAL;
@@ -249,55 +333,193 @@ struct ib_umem *__ib_umem_get(struct ib_
 
 	lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
 
+#ifdef HAVE_ATOMIC_PINNED_VM
 	new_pinned = atomic64_add_return(npages, &mm->pinned_vm);
 	if (new_pinned > lock_limit && !capable(CAP_IPC_LOCK)) {
+#else
+	down_write(&mm->mmap_sem);
+#ifdef HAVE_PINNED_VM
+	if (check_add_overflow(mm->pinned_vm, npages, &new_pinned) ||
+	    (new_pinned > lock_limit && !capable(CAP_IPC_LOCK))) {
+#else
+	current->mm->locked_vm += npages;
+	if ((current->mm->locked_vm > lock_limit) && !capable(CAP_IPC_LOCK)) {
+#endif /* HAVE_PINNED_VM */
+#endif /* HAVE_ATOMIC_PINNED_VM */
+
+#ifdef HAVE_ATOMIC_PINNED_VM
 		atomic64_sub(npages, &mm->pinned_vm);
+#else
+		up_write(&mm->mmap_sem);
+#ifndef HAVE_PINNED_VM
+		current->mm->locked_vm -= npages;
+#endif /* HAVE_PINNED_VM */
+#endif /* HAVE_ATOMIC_PINNED_VM */
 		ret = -ENOMEM;
 		goto out;
 	}
 
+#ifndef HAVE_ATOMIC_PINNED_VM
+#ifdef HAVE_PINNED_VM
+	mm->pinned_vm = new_pinned;
+#endif /* HAVE_PINNED_VM */
+	up_write(&mm->mmap_sem);
+#endif /* HAVE_ATOMIC_PINNED_VM */
 	cur_base = addr & PAGE_MASK;
 
+#ifndef HAVE_SG_ALLOC_TABLE_FROM_PAGES
 	ret = sg_alloc_table(&umem->sg_head, npages, GFP_KERNEL);
 	if (ret)
 		goto vma;
+#endif
+#ifdef HAVE_GET_USER_PAGES_GUP_FLAGS
+       if (!umem->writable)
+       	gup_flags |= FOLL_FORCE;
+#endif
 
-	if (!umem->writable)
-		gup_flags |= FOLL_FORCE;
-
+#ifndef HAVE_SG_ALLOC_TABLE_FROM_PAGES
 	sg = umem->sg_head.sgl;
+#endif
 
 	while (npages) {
 		cond_resched();
+#ifdef HAVE_UNPIN_USER_PAGES_DIRTY_LOCK_EXPORTED
 		ret = pin_user_pages_fast(cur_base,
 					  min_t(unsigned long, npages,
 						PAGE_SIZE /
 						sizeof(struct page *)),
 					  gup_flags | FOLL_LONGTERM, page_list);
+		if (ret < 0)
+			goto umem_release;
+#else
+		down_read(&mm->mmap_sem);
+#ifdef HAVE_FOLL_LONGTERM
+		ret = get_user_pages(cur_base,
+				     min_t(unsigned long, npages,
+					   PAGE_SIZE / sizeof (struct page *)),
+				     gup_flags | FOLL_LONGTERM,
+				     page_list, NULL);
+#elif defined(HAVE_GET_USER_PAGES_LONGTERM)
+		ret = get_user_pages_longterm(cur_base,
+			min_t(unsigned long, npages,
+			PAGE_SIZE / sizeof (struct page *)),
+			gup_flags, page_list, NULL);
+#elif defined(HAVE_GET_USER_PAGES_8_PARAMS)
+		ret = get_user_pages(current, current->mm, cur_base,
+				     min_t(unsigned long, npages,
+					   PAGE_SIZE / sizeof (struct page *)),
+				     1, !umem->writable, page_list, vma_list);
+#else
+#ifdef HAVE_GET_USER_PAGES_7_PARAMS
+		ret = get_user_pages(current, current->mm, cur_base,
+#else
+		ret = get_user_pages(cur_base,
+#endif
+				min_t(unsigned long, npages,
+					PAGE_SIZE / sizeof (struct page *)),
+#ifdef HAVE_GET_USER_PAGES_GUP_FLAGS
+				gup_flags, page_list, vma_list);
+#else
+				1, !umem->writable, page_list, vma_list);
+#endif
+#endif /*HAVE_FOLL_LONGTERM*/
+
 		if (ret < 0) {
+#ifdef HAVE_GET_USER_PAGES_GUP_FLAGS
 			pr_debug("%s: failed to get user pages, nr_pages=%lu, flags=%u\n", __func__,
 			       min_t(unsigned long, npages,
 				     PAGE_SIZE / sizeof(struct page *)),
 			       gup_flags);
+#else
+			pr_debug("%s: failed to get user pages, nr_pages=%lu\n", __func__,
+			       min_t(unsigned long, npages,
+				     PAGE_SIZE / sizeof(struct page *)));
+#endif
+#ifndef HAVE_UNPIN_USER_PAGES_DIRTY_LOCK_EXPORTED
+			up_read(&mm->mmap_sem);
+#endif
 			goto umem_release;
 		}
+#endif /*HAVE_UNPIN_USER_PAGES_DIRTY_LOCK_EXPORTED*/
 
 		cur_base += ret * PAGE_SIZE;
 		npages   -= ret;
-
+#ifdef HAVE_SG_ALLOC_TABLE_FROM_PAGES
+		sg = __sg_alloc_table_from_pages(
+			&umem->sg_head, page_list, ret, 0, ret << PAGE_SHIFT,
+#else
 		sg = ib_umem_add_sg_table(sg, page_list, ret,
-			dma_get_max_seg_size(device->dma_device),
+#endif
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
+       		dma_get_max_seg_size(device->dma_device),
+#else
+			dma_get_max_seg_size(context->device->dma_device),
+#endif
+#ifdef HAVE_SG_ALLOC_TABLE_FROM_PAGES
+			sg, npages,
+			GFP_KERNEL);
+		umem->sg_nents = umem->sg_head.nents;
+		if (IS_ERR(sg)) {
+#ifdef HAVE_UNPIN_USER_PAGES_DIRTY_LOCK_EXPORTED
+			unpin_user_pages_dirty_lock(page_list, ret, 0);
+#elif defined(HAVE_PUT_USER_PAGES_DIRTY_LOCK_3_PARAMS)
+			put_user_pages_dirty_lock(page_list, ret, 0);
+#elif defined(HAVE_PUT_USER_PAGES_DIRTY_LOCK_2_PARAMS)
+			for (index = 0; index < ret; index++)
+				put_user_page(page_list[index]);
+#else
+			for (index = 0; index < ret; index++)
+				put_page(page_list[index]);
+#endif /*HAVE_UNPIN_USER_PAGES_DIRTY_LOCK_EXPORTED*/
+			ret = PTR_ERR(sg);
+			goto umem_release;
+			}
+#else
 			&umem->sg_nents);
+#endif
+
+#if !defined(HAVE_FOLL_LONGTERM) && !defined(HAVE_GET_USER_PAGES_LONGTERM)
+		/* Continue to hold the mmap_sem as vma_list access
+		 *               * needs to be protected.
+		 *                                */
+		for (i = 0; i < ret && umem->hugetlb; i++) {
+			if (vma_list && !is_vm_hugetlb_page(vma_list[i]))
+				umem->hugetlb = 0;
+		}
+#endif
+#ifndef HAVE_UNPIN_USER_PAGES_DIRTY_LOCK_EXPORTED
+		up_read(&mm->mmap_sem);
+#endif
 	}
 
+#ifndef HAVE_SG_ALLOC_TABLE_FROM_PAGES
 	sg_mark_end(sg);
+#endif
 
 	if (!(access & IB_ACCESS_DISABLE_RELAXED_ORDERING))
 		dma_attr |= DMA_ATTR_WEAK_ORDERING;
 
-	umem->nmap =
-		ib_dma_map_sg_attrs(device, umem->sg_head.sgl, umem->sg_nents,
-				    DMA_BIDIRECTIONAL, dma_attr);
+#ifndef DMA_ATTR_WRITE_BARRIER
+	umem->nmap = ib_dma_map_sg(
+#else
+	umem->nmap = ib_dma_map_sg_attrs(
+#endif
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
+					device,
+#else
+					context->device,
+#endif
+					umem->sg_head.sgl,
+ 				  	umem->sg_nents,
+					DMA_BIDIRECTIONAL
+#ifdef DMA_ATTR_WRITE_BARRIER
+#ifdef HAVE_STRUCT_DMA_ATTRS
+                                  , &attrs
+#else
+                                  , dma_attrs
+#endif //HAVE_STRUCT_DMA_ATTRS
+#endif //DMA_ATTR_WRITE_BARRIER
+				  );
 
 	if (!umem->nmap) {
 		pr_err("%s: failed to map scatterlist, npages=%lu\n", __func__,
@@ -310,7 +532,11 @@ struct ib_umem *__ib_umem_get(struct ib_
 	goto out;
 
 umem_release:
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 	__ib_umem_release(device, umem, 0);
+#else
+	__ib_umem_release(context->device, umem, 0);
+#endif
 	/*
 	 * If the address belongs to peer memory client, then the first
 	 * call to get_user_pages will fail. In this case, try to get
@@ -329,9 +555,24 @@ umem_release:
 		ret = 0;
 		goto out;
 	}
+
 vma:
+#ifdef HAVE_ATOMIC_PINNED_VM
 	atomic64_sub(ib_umem_num_pages(umem), &mm->pinned_vm);
+#else
+	down_write(&mm->mmap_sem);
+#ifdef HAVE_PINNED_VM
+	mm->pinned_vm -= ib_umem_num_pages(umem);
+#else
+	mm->locked_vm -= ib_umem_num_pages(umem);
+#endif /* HAVE_PINNED_VM */
+	up_write(&mm->mmap_sem);
+#endif /* HAVE_ATOMIC_PINNED_VM */
 out:
+#if !defined(HAVE_FOLL_LONGTERM) && !defined(HAVE_GET_USER_PAGES_LONGTERM)
+	if (vma_list)
+		free_page((unsigned long) vma_list);
+#endif
 	free_page((unsigned long) page_list);
 umem_kfree:
 	if (ret) {
@@ -341,19 +582,36 @@ umem_kfree:
 	return ret ? ERR_PTR(ret) : umem;
 }
 
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 struct ib_umem *ib_umem_get(struct ib_device *device, unsigned long addr,
+#else
+struct ib_umem *ib_umem_get(struct ib_udata *udata, unsigned long addr,
+#endif
 			    size_t size, int access)
 {
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 	return __ib_umem_get(device, addr, size, access, 0);
+#else
+	return __ib_umem_get(udata, addr, size, access, 0);
+#endif
 }
 EXPORT_SYMBOL(ib_umem_get);
 
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 struct ib_umem *ib_umem_get_peer(struct ib_device *device, unsigned long addr,
+#else
+struct ib_umem *ib_umem_get_peer(struct ib_udata *udata, unsigned long addr,
+#endif
 				 size_t size, int access,
 				 unsigned long peer_mem_flags)
 {
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 	return __ib_umem_get(device, addr, size, access,
 			     IB_PEER_MEM_ALLOW | peer_mem_flags);
+#else
+	return __ib_umem_get(udata, addr, size, access,
+			     IB_PEER_MEM_ALLOW | peer_mem_flags);
+#endif
 }
 EXPORT_SYMBOL(ib_umem_get_peer);
 
@@ -365,14 +623,30 @@ void ib_umem_release(struct ib_umem *ume
 {
 	if (!umem)
 		return;
+#ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	if (umem->is_odp)
 		return ib_umem_odp_release(to_ib_umem_odp(umem));
+#endif
 
 	if (umem->is_peer)
 		return ib_peer_umem_release(umem);
+#ifdef HAVE_MMU_NOTIFIER_OPS_HAS_FREE_NOTIFIER
 	__ib_umem_release(umem->ibdev, umem, 1);
-
+#else
+	__ib_umem_release(umem->context->device, umem, 1);
+#endif
+ 
+#ifdef HAVE_ATOMIC_PINNED_VM
 	atomic64_sub(ib_umem_num_pages(umem), &umem->owning_mm->pinned_vm);
+#else
+	down_write(&umem->owning_mm->mmap_sem);
+#ifdef HAVE_PINNED_VM
+	umem->owning_mm->pinned_vm -= ib_umem_num_pages(umem);
+#else
+	umem->owning_mm->locked_vm -= ib_umem_num_pages(umem);
+#endif /* HAVE_PINNED_VM */
+	up_write(&umem->owning_mm->mmap_sem);
+#endif /*HAVE_ATOMIC_PINNED_VM*/
 	mmdrop(umem->owning_mm);
 	kfree(umem);
 }
